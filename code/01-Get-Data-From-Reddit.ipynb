{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beautiful-monkey",
   "metadata": {},
   "source": [
    "# Capstone Project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-rescue",
   "metadata": {},
   "source": [
    "### Author: Ramesh Babu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-camcorder",
   "metadata": {},
   "source": [
    "#### Batch DSI-11302020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-optimization",
   "metadata": {},
   "source": [
    "### 01 - Data Collection from Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-sauce",
   "metadata": {},
   "source": [
    "##### Instructions: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "virtual-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all necessary packages\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import requests \n",
    "import json \n",
    "import csv \n",
    "import time\n",
    "import datetime\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "logical-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function will return 100 rows from given subreddit for a utc time given\n",
    "def get_100_df(subreddit, before):\n",
    "    params = {\n",
    "    'subreddit' : subreddit,\n",
    "    'fields' : ['subreddit','title', 'selftext', 'id', 'full_link', 'created_utc'],\n",
    "    'size' : '100',\n",
    "    'before' : before\n",
    "    }\n",
    "    \n",
    "    base_url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    try:\n",
    "        res = requests.get(base_url, params)    \n",
    "        raw_data = res.json()\n",
    "        posts = raw_data['data']\n",
    "        df = pd.DataFrame(posts)\n",
    "    except ValueError:  # includes simplejson.decoder.JSONDecodeError\n",
    "        print ('Decoding JSON has failed')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "secure-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function loops through 100 times and wait for 3 seconds for every pull\n",
    "def start_process(subreddit, before):\n",
    "    start_time = time.time()\n",
    "    df_final = pd.DataFrame(columns=['subreddit','title','selftext','id','created_utc','full_link'])\n",
    "    # for i in range(100):\n",
    "    for i in range(1):\n",
    "        print(f\" started .. {i}\")\n",
    "        df_raw = get_100_df(subreddit,before )\n",
    "        df_temp = df_raw[['subreddit','title','selftext','id','created_utc','full_link']]\n",
    "        df_final = df_final.append(df_temp)\n",
    "        before = df_temp['created_utc'].min(axis = 0) \n",
    "        print(time.strftime('%d/%b/%y %I:%m %p', time.localtime(before)))\n",
    "        time.sleep(3)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-impression",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "better-tracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todays date:  2021-03-03 18:03:18.075895 and its equivalent UTC 1614812598\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "30 days back date:  2021-02-01 18:03:18.075895 and its equivalent UTC 1612220598\n",
      "30 days back date:  2021-01-02 18:03:18.075895 and its equivalent UTC 1609628598\n",
      "30 days back date:  2020-12-03 18:03:18.075895 and its equivalent UTC 1607036598\n",
      "30 days back date:  2020-11-03 18:03:18.075895 and its equivalent UTC 1604444598\n",
      "30 days back date:  2020-10-04 18:03:18.075895 and its equivalent UTC 1601848998\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "## this cell will give the current datatime in UTC and 5 times of 30 days back \n",
    "from datetime import timedelta, datetime\n",
    "now = datetime.now()\n",
    "print(f\"Todays date:  {now} and its equivalent UTC {now.strftime('%s')}\")\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "for i in range(5):\n",
    "    now += timedelta(days=-30)\n",
    "    print(f\"30 days back date:  {now} and its equivalent UTC {now.strftime('%s')}\")\n",
    "\n",
    "\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "regular-registrar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " started .. 0\n",
      "02/Mar/21 11:03 AM\n",
      "--- 3.4765677452087402 seconds ---\n",
      "1614704119\n"
     ]
    }
   ],
   "source": [
    "# loop through below subriddit topics\n",
    "# PoliticalDiscussion\n",
    "# MachineLearning\n",
    "# books\n",
    "## Cooking\n",
    "## movies\n",
    "# todays date : 1614812598 (  2021-03-03 18:03:18.075895)\n",
    "\n",
    "subreddit = 'books'\n",
    "before = '1614812598'\n",
    "df_final_list = start_process(subreddit, before)\n",
    "\n",
    "min = df_final_list['created_utc'].min(axis = 0) \n",
    "print(min)\n",
    "file_name = '../data/' + subreddit+\"_\"+str(min) + \".csv\"\n",
    "df_final_list.to_csv(file_name)\n",
    "\n",
    "# above saved file name has the oldest UTC time for next pull \n",
    "# 1609733073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acquired-seven",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " started .. 0\n",
      "26/Feb/21 09:02 AM\n",
      "--- 3.3441288471221924 seconds ---\n",
      "1614348717\n"
     ]
    }
   ],
   "source": [
    "# loop through below subriddit topics\n",
    "# PoliticalDiscussion\n",
    "# MachineLearning\n",
    "# books\n",
    "## Cooking\n",
    "## movies\n",
    "# todays date : 1614812598 (  2021-03-03 18:03:18.075895)\n",
    "\n",
    "subreddit = 'PoliticalDiscussion'\n",
    "before = '1614812598'\n",
    "df_final_list = start_process(subreddit, before)\n",
    "\n",
    "min = df_final_list['created_utc'].min(axis = 0) \n",
    "print(min)\n",
    "file_name = '../data/' + subreddit+\"_\"+str(min) + \".csv\"\n",
    "df_final_list.to_csv(file_name)\n",
    "\n",
    "# above saved file name has the oldest UTC time for next pull \n",
    "# 1609733073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "appropriate-newton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " started .. 0\n",
      "27/Feb/21 03:02 AM\n",
      "--- 3.4161159992218018 seconds ---\n",
      "1614415561\n"
     ]
    }
   ],
   "source": [
    "# loop through below subriddit topics\n",
    "# PoliticalDiscussion\n",
    "# MachineLearning\n",
    "# books\n",
    "## Cooking\n",
    "## movies\n",
    "# todays date : 1614812598 (  2021-03-03 18:03:18.075895)\n",
    "\n",
    "subreddit = 'MachineLearning'\n",
    "before = '1614812598'\n",
    "df_final_list = start_process(subreddit, before)\n",
    "\n",
    "min = df_final_list['created_utc'].min(axis = 0) \n",
    "print(min)\n",
    "file_name = '../data/' + subreddit+\"_\"+str(min) + \".csv\"\n",
    "df_final_list.to_csv(file_name)\n",
    "\n",
    "# above saved file name has the oldest UTC time for next pull \n",
    "# 1609733073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "creative-blank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " started .. 0\n",
      "27/Feb/21 09:02 PM\n",
      "--- 4.312026739120483 seconds ---\n",
      "1614479417\n"
     ]
    }
   ],
   "source": [
    "# loop through below subriddit topics\n",
    "# PoliticalDiscussion\n",
    "# MachineLearning\n",
    "# books\n",
    "## Cooking\n",
    "## movies\n",
    "# todays date : 1614812598 (  2021-03-03 18:03:18.075895)\n",
    "\n",
    "subreddit = 'Cooking'\n",
    "before = '1614812598'\n",
    "df_final_list = start_process(subreddit, before)\n",
    "\n",
    "min = df_final_list['created_utc'].min(axis = 0) \n",
    "print(min)\n",
    "file_name = '../data/' + subreddit+\"_\"+str(min) + \".csv\"\n",
    "df_final_list.to_csv(file_name)\n",
    "\n",
    "# above saved file name has the oldest UTC time for next pull \n",
    "# 1609733073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "other-channel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " started .. 0\n",
      "02/Mar/21 05:03 PM\n",
      "--- 3.397855043411255 seconds ---\n",
      "1614724349\n"
     ]
    }
   ],
   "source": [
    "# loop through below subriddit topics\n",
    "# PoliticalDiscussion\n",
    "# MachineLearning\n",
    "# books\n",
    "## Cooking\n",
    "## movies\n",
    "# todays date : 1614812598 (  2021-03-03 18:03:18.075895)\n",
    "\n",
    "subreddit = 'movies'\n",
    "before = '1614812598'\n",
    "df_final_list = start_process(subreddit, before)\n",
    "\n",
    "min = df_final_list['created_utc'].min(axis = 0) \n",
    "print(min)\n",
    "file_name = '../data/' + subreddit+\"_\"+str(min) + \".csv\"\n",
    "df_final_list.to_csv(file_name)\n",
    "\n",
    "# above saved file name has the oldest UTC time for next pull \n",
    "# 1609733073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-chaos",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
